{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce110000-1111-2222-3333-ffffff000000",
   "metadata": {
    "name": "cell1"
   },
   "source": [
    "# Audio/Video Transcription with OpenAI Whisper\n",
    "\n",
    "In this Notebook on **Container Runtime**, we will **process audio and video files** to generate accurate transcriptions using OpenAI's Whisper model. This notebook leverages GPU acceleration for faster processing and stores results in structured Snowflake tables for analysis and search.\n",
    "\n",
    "## What we'll accomplish:\n",
    "- **Audio/Video Processing**: Convert various media formats to audio for transcription\n",
    "- **Speech-to-Text**: Use OpenAI Whisper for accurate transcription with language detection\n",
    "- **Batch Processing**: Process multiple files efficiently with progress tracking\n",
    "- **Data Storage**: Store transcriptions with metadata in Snowflake tables\n",
    "- **JSON Output**: Structured transcripts with speaker segments and timestamps\n",
    "\n",
    "**Why is Container Runtime needed?**\n",
    "Since we have audio and video files, we need to install OpenAI Whisper and FFmpeg for media processing. FFmpeg is required for audio extraction from video files and cannot be installed in standard Warehouse compute. We also use GPU compute here, which makes transcription much faster.\n",
    "\n",
    "**‚ö†Ô∏è IMPORTANT**: This notebook installs all packages via pip and doesn't require environment.yml. Make sure External Access Integrations are enabled!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce110000-1111-2222-3333-ffffff000001",
   "metadata": {
    "name": "cell2"
   },
   "source": [
    "## OpenAI Whisper\n",
    "\n",
    "Whisper is a general-purpose speech recognition model from OpenAI. It's trained on a large dataset of diverse audio and is also a multitasking model that can perform multilingual speech recognition, speech translation, and language identification.\n",
    "\n",
    "**Key Benefits:**\n",
    "- **Accuracy**: State-of-the-art transcription quality\n",
    "- **Multilingual**: Supports 99+ languages\n",
    "- **Robust**: Handles various audio qualities and accents\n",
    "- **GPU Accelerated**: Fast processing on Snowflake's GPU compute\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce110000-1111-2222-3333-ffffff000002",
   "metadata": {
    "name": "cell3"
   },
   "source": [
    "## Let's get started!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Configuration\n",
    "\n",
    "Set your transcription options here before running the rest of the notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################\n",
    "# TRANSCRIPTION CONFIGURATION\n",
    "####################################\n",
    "\n",
    "# Whisper Model Selection\n",
    "# Options: \"tiny\", \"base\", \"small\", \"medium\", \"large\"\n",
    "# - tiny: Fastest, least accurate (~39x realtime)\n",
    "# - base: Good balance (default, ~16x realtime)\n",
    "# - small: Better accuracy (~6x realtime)\n",
    "# - medium: High accuracy (~2x realtime)\n",
    "# - large: Best accuracy (~1x realtime)\n",
    "WHISPER_MODEL = \"base\"\n",
    "\n",
    "# Speaker Diarization (identifies different speakers)\n",
    "# Set to True to enable speaker identification (requires additional processing time)\n",
    "# Set to False for faster transcription without speaker separation\n",
    "ENABLE_SPEAKER_DIARIZATION = False\n",
    "\n",
    "# Batch Processing\n",
    "# Number of files to process before showing progress update\n",
    "PROGRESS_UPDATE_INTERVAL = 5\n",
    "\n",
    "# Output Options\n",
    "# Include file path in results (useful for organizing large batches)\n",
    "INCLUDE_FILE_PATH = True\n",
    "\n",
    "print(\"‚úÖ Configuration loaded:\")\n",
    "print(f\"   üìä Whisper Model: {WHISPER_MODEL}\")\n",
    "print(f\"   üë• Speaker Diarization: {'ENABLED' if ENABLE_SPEAKER_DIARIZATION else 'DISABLED'}\")\n",
    "print(f\"   üìà Progress Updates: Every {PROGRESS_UPDATE_INTERVAL} files\")\n",
    "print(f\"   üìÅ Include File Paths: {INCLUDE_FILE_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce110000-1111-2222-3333-ffffff000003",
   "metadata": {
    "language": "python",
    "name": "install_packages"
   },
   "outputs": [],
   "source": [
    "# Install all required packages since environment.yml is minimal\n",
    "print(\"üîß Installing required packages...\")\n",
    "print(\"This may take several minutes...\")\n",
    "\n",
    "!pip install openai-whisper pandas\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from snowflake.core import Root\n",
    "from snowflake.snowpark.context import get_active_session\n",
    "from snowflake.snowpark.functions import col\n",
    "from snowflake.snowpark.types import StructType, StructField, StringType, FloatType, IntegerType, TimestampType\n",
    "import glob\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "session = get_active_session()\n",
    "root = Root(session)\n",
    "\n",
    "# Add a query tag to the session for monitoring\n",
    "session.query_tag = {\"origin\":\"sf_se\", \n",
    "                     \"name\":\"audio_video_transcription\", \n",
    "                     \"version\":{\"major\":1, \"minor\":0},\n",
    "                     \"attributes\":{\"is_quickstart\":0, \"source\":\"notebook\"}}\n",
    "\n",
    "# Set session context \n",
    "session.use_role(\"SYSADMIN\")\n",
    "\n",
    "# Print the current role, warehouse, and database/schema\n",
    "print(f\"Role: {session.get_current_role()}\")\n",
    "print(f\"Warehouse: {session.get_current_warehouse()}\")\n",
    "print(f\"Database.Schema: {session.get_fully_qualified_current_schema()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce110000-1111-2222-3333-ffffff000004",
   "metadata": {
    "name": "cell5"
   },
   "source": [
    "## 1. Prerequisites & Setup\n",
    "\n",
    "**üö® CRITICAL: External Access Integrations Must Be Enabled**\n",
    "\n",
    "Before running any installation commands, you MUST enable external access integrations:\n",
    "\n",
    "### **Steps to Enable External Access:**\n",
    "1. Click the **‚öôÔ∏è Settings** icon in the top-right of this notebook\n",
    "2. In the settings panel, find **\"External Access Integrations\"**\n",
    "3. **Toggle ON** these integrations:\n",
    "   - ‚úÖ `TRANSCRIPTION_PYPI_ACCESS_INTEGRATION`\n",
    "   - ‚úÖ `TRANSCRIPTION_ALLOW_ALL_INTEGRATION`\n",
    "4. Click **\"Save\"** or close the settings panel\n",
    "\n",
    "### **Other Prerequisites:**\n",
    "- Upload media files to the `AUDIO_VIDEO_STAGE` in Snowflake\n",
    "- Ensure you're using a GPU compute pool for faster processing\n",
    "\n",
    "**‚ö†Ô∏è If you skip enabling external access integrations, package installation will fail!**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce110000-1111-2222-3333-ffffff000005",
   "metadata": {
    "name": "cell6"
   },
   "source": [
    "Verify FFmpeg is available (pre-installed in Container Runtime):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce110000-1111-2222-3333-ffffff000006",
   "metadata": {
    "language": "python",
    "name": "install_ffmpeg"
   },
   "outputs": [],
   "source": [
    "# Check if FFmpeg is available (usually pre-installed in Container Runtime)\n",
    "import subprocess\n",
    "\n",
    "try:\n",
    "    result = subprocess.run(['ffmpeg', '-version'], capture_output=True, text=True, check=True)\n",
    "    print(\"‚úÖ FFmpeg is already installed!\")\n",
    "    print(f\"   Version: {result.stdout.split()[2]}\")\n",
    "except:\n",
    "    print(\"‚ö†Ô∏è  FFmpeg not found. Attempting installation...\")\n",
    "    try:\n",
    "        # Install if not present\n",
    "        subprocess.run(['apt-get', 'update'], check=True, capture_output=True)\n",
    "        subprocess.run(['apt-get', 'install', '-y', 'ffmpeg'], check=True, capture_output=True)\n",
    "        print(\"‚úÖ FFmpeg installed successfully!\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to install FFmpeg: {e}\")\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce110000-1111-2222-3333-ffffff000007",
   "metadata": {
    "language": "python",
    "name": "which_ffmpeg"
   },
   "outputs": [],
   "source": [
    "# Verify ffprobe is also available (needed for duration detection)\n",
    "try:\n",
    "    result = subprocess.run(['ffprobe', '-version'], capture_output=True, text=True, check=True)\n",
    "    print(\"‚úÖ ffprobe is available\")\n",
    "except:\n",
    "    print(\"‚ö†Ô∏è ffprobe not found (should be installed with ffmpeg)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce110000-1111-2222-3333-ffffff000008",
   "metadata": {
    "language": "python",
    "name": "version_ffmpeg"
   },
   "outputs": [],
   "source": [
    "# Display FFmpeg capabilities\n",
    "!ffmpeg -version | head -3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce110000-1111-2222-3333-ffffff000009",
   "metadata": {
    "name": "cell10"
   },
   "source": [
    "Now install OpenAI Whisper and additional dependencies:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce110000-1111-2222-3333-ffffff000010",
   "metadata": {
    "language": "python",
    "name": "install_whisper_moviepy"
   },
   "outputs": [],
   "source": [
    "# Install required packages for transcription\n",
    "# CRITICAL: External Access Integrations must be enabled in notebook settings\n",
    "\n",
    "print(\"üì¶ Installing required packages...\")\n",
    "print(\"üîÑ Step 1: Installing OpenAI Whisper...\")\n",
    "\n",
    "try:\n",
    "    import subprocess\n",
    "    import sys\n",
    "    \n",
    "    # Install whisper first\n",
    "    result = subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"openai-whisper\"], \n",
    "                          capture_output=True, text=True, check=True)\n",
    "    print(\"‚úÖ OpenAI Whisper installed successfully\")\n",
    "    \n",
    "except subprocess.CalledProcessError as e:\n",
    "    print(f\"‚ùå Failed to install OpenAI Whisper: {e}\")\n",
    "    print(f\"Error output: {e.stderr}\")\n",
    "    print(\"üö® Make sure External Access Integrations are enabled in notebook settings!\")\n",
    "    raise\n",
    "\n",
    "print(\"‚úÖ Package installation complete!\")\n",
    "print(\"‚ÑπÔ∏è  Video processing uses FFmpeg CLI (more reliable in Container Runtime)\")\n",
    "print(\"‚ÑπÔ∏è  Speaker diarization is available as an optional enhancement.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce110000-1111-2222-3333-ffffff000011",
   "metadata": {
    "name": "cell12"
   },
   "source": [
    "**üîç Troubleshooting Installation Issues:**\n",
    "\n",
    "If the installation above failed:\n",
    "1. **Check External Access Integrations** - Go to notebook settings and ensure they're enabled\n",
    "2. **Restart the notebook** - Sometimes a fresh start helps\n",
    "3. **Try manual installation** - Run individual pip commands in separate cells\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce110000-1111-2222-3333-ffffff000012",
   "metadata": {
    "name": "cell13"
   },
   "source": [
    "## 2. Load Whisper Model\n",
    "\n",
    "Load the Whisper model configured in the Configuration cell above (default: 'base'). You can change the model by updating `WHISPER_MODEL` in the configuration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce110000-1111-2222-3333-ffffff000013",
   "metadata": {
    "language": "python",
    "name": "cell14"
   },
   "outputs": [],
   "source": [
    "# Import required libraries and test installations\n",
    "print(\"üìö Testing library imports...\")\n",
    "\n",
    "# Test Whisper import\n",
    "try:\n",
    "    import whisper\n",
    "    print(\"   ‚úÖ OpenAI Whisper imported successfully\")\n",
    "    WHISPER_AVAILABLE = True\n",
    "except ImportError as e:\n",
    "    print(f\"   ‚ùå Error importing Whisper: {e}\")\n",
    "    print(\"   üö® SOLUTION: Enable External Access Integrations in notebook settings\")\n",
    "    print(\"   Then restart the notebook and try again\")\n",
    "    WHISPER_AVAILABLE = False\n",
    "\n",
    "# Video processing uses FFmpeg CLI directly\n",
    "print(\"   ‚úÖ Video processing available via FFmpeg CLI\")\n",
    "\n",
    "# Check for optional speaker diarization packages\n",
    "try:\n",
    "    from pyannote.audio import Pipeline\n",
    "    import torch\n",
    "    print(\"   ‚úÖ PyAnnote.audio imported\")\n",
    "    DIARIZATION_AVAILABLE = True\n",
    "except ImportError as e:\n",
    "    print(f\"   ‚ÑπÔ∏è  PyAnnote.audio not available (this is optional)\")\n",
    "    print(\"   Speaker diarization will be skipped. Only basic transcription will be performed.\")\n",
    "    DIARIZATION_AVAILABLE = False\n",
    "\n",
    "import json\n",
    "\n",
    "# Check if we can proceed\n",
    "if not WHISPER_AVAILABLE:\n",
    "    print(\"\\n‚ùå Cannot proceed without Whisper. Please:\")\n",
    "    print(\"   1. Go to notebook Settings (‚öôÔ∏è icon)\")\n",
    "    print(\"   2. Enable External Access Integrations\")\n",
    "    print(\"   3. Restart this notebook\")\n",
    "    print(\"   4. Re-run the installation cell\")\n",
    "    raise ImportError(\"Whisper is required but not available\")\n",
    "\n",
    "# Load whisper model\n",
    "if WHISPER_AVAILABLE:\n",
    "    print(f\"\\nü§ñ Loading Whisper model: '{WHISPER_MODEL}'...\")\n",
    "    try:\n",
    "        model = whisper.load_model(WHISPER_MODEL)\n",
    "        print(f\"‚úÖ Whisper model '{WHISPER_MODEL}' loaded successfully!\")\n",
    "        print(f\"   Model: {model.dims}\")\n",
    "        print(f\"   Device: {next(model.parameters()).device}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading Whisper model: {e}\")\n",
    "        print(\"   This might be a GPU/memory issue. Try restarting the notebook.\")\n",
    "        raise\n",
    "else:\n",
    "    print(\"‚ùå Skipping model loading - Whisper not available\")\n",
    "    model = None\n",
    "\n",
    "# Load speaker diarization pipeline (optional, based on configuration)\n",
    "diarization_pipeline = None\n",
    "if ENABLE_SPEAKER_DIARIZATION and DIARIZATION_AVAILABLE:\n",
    "    print(\"\\nüë• Loading speaker diarization pipeline...\")\n",
    "    try:\n",
    "        # Note: This requires a HuggingFace token for some models\n",
    "        # If you get authentication errors, you'll need to provide a token\n",
    "        diarization_pipeline = Pipeline.from_pretrained(\"pyannote/speaker-diarization-3.1\")\n",
    "        print(\"‚úÖ Speaker diarization pipeline loaded!\")\n",
    "        print(\"   This will add speaker identification to transcriptions\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  Could not load diarization pipeline: {e}\")\n",
    "        print(\"   Continuing without speaker diarization...\")\n",
    "        DIARIZATION_AVAILABLE = False\n",
    "elif ENABLE_SPEAKER_DIARIZATION and not DIARIZATION_AVAILABLE:\n",
    "    print(\"\\n‚ö†Ô∏è  Speaker diarization is ENABLED in config but PyAnnote is not available\")\n",
    "    print(\"   Install PyAnnote to enable speaker diarization:\")\n",
    "    print(\"   !pip install pyannote.audio torch torchaudio\")\n",
    "else:\n",
    "    print(\"\\n‚ÑπÔ∏è  Speaker diarization is DISABLED in configuration\")\n",
    "    print(\"   Set ENABLE_SPEAKER_DIARIZATION = True to enable it\")\n",
    "    DIARIZATION_AVAILABLE = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce110000-1111-2222-3333-ffffff000014",
   "metadata": {
    "name": "cell15"
   },
   "source": [
    "## 3. Download Media Files\n",
    "\n",
    "Download all audio and video files from the Snowflake stage to process locally.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce110000-1111-2222-3333-ffffff000015",
   "metadata": {
    "language": "python",
    "name": "download_media"
   },
   "outputs": [],
   "source": [
    "# Create local directory for media files\n",
    "os.makedirs('media_files', exist_ok=True)\n",
    "\n",
    "# Download all files from the audio/video stage\n",
    "print(\"Downloading files from stage...\")\n",
    "try:\n",
    "    files = session.file.get('@TRANSCRIPTION_DB.TRANSCRIPTION_SCHEMA.AUDIO_VIDEO_STAGE/', 'media_files/')\n",
    "    print(f\"‚úÖ Downloaded {len(files)} files\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  Error downloading files: {e}\")\n",
    "    print(\"Make sure you have uploaded files to the AUDIO_VIDEO_STAGE first!\")\n",
    "    files = []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce110000-1111-2222-3333-ffffff000016",
   "metadata": {
    "name": "cell17"
   },
   "source": [
    "## 4. Define Helper Functions\n",
    "\n",
    "Create functions to handle audio extraction from video files and transcription processing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce110000-1111-2222-3333-ffffff000017",
   "metadata": {
    "language": "python",
    "name": "helper_functions"
   },
   "outputs": [],
   "source": [
    "def extract_audio_from_video(video_path, audio_path):\n",
    "    \"\"\"\n",
    "    Extract audio from video file using FFmpeg CLI (more reliable in Container Runtime)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        import subprocess\n",
    "        \n",
    "        # Get video duration using ffprobe\n",
    "        duration_cmd = [\n",
    "            'ffprobe',\n",
    "            '-v', 'error',\n",
    "            '-show_entries', 'format=duration',\n",
    "            '-of', 'default=noprint_wrappers=1:nokey=1',\n",
    "            video_path\n",
    "        ]\n",
    "        \n",
    "        try:\n",
    "            duration_result = subprocess.run(duration_cmd, capture_output=True, text=True, check=True)\n",
    "            duration = float(duration_result.stdout.strip())\n",
    "        except:\n",
    "            duration = 0\n",
    "        \n",
    "        # Extract audio using FFmpeg\n",
    "        extract_cmd = [\n",
    "            'ffmpeg',\n",
    "            '-y',  # Overwrite output file\n",
    "            '-i', video_path,  # Input file\n",
    "            '-vn',  # No video\n",
    "            '-acodec', 'pcm_s16le',  # Audio codec for WAV\n",
    "            '-ar', '16000',  # Sample rate (Whisper compatible)\n",
    "            '-ac', '1',  # Mono audio\n",
    "            audio_path\n",
    "        ]\n",
    "        \n",
    "        result = subprocess.run(extract_cmd, capture_output=True, text=True, check=False)\n",
    "        \n",
    "        if result.returncode == 0 and os.path.exists(audio_path):\n",
    "            return True, duration\n",
    "        else:\n",
    "            print(f\"FFmpeg extraction failed: {result.stderr[:200]}\")\n",
    "            return False, 0\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting audio from {video_path}: {e}\")\n",
    "        return False, 0\n",
    "\n",
    "def get_file_size(file_path):\n",
    "    \"\"\"Get file size in bytes\"\"\"\n",
    "    try:\n",
    "        return os.path.getsize(file_path)\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "def transcribe_media_file(file_path):\n",
    "    \"\"\"\n",
    "    Transcribe audio or video files using Whisper with optional speaker diarization\n",
    "    Returns: (transcript, language, processing_time, audio_duration, transcript_with_speakers, speaker_count)\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        # Determine file type\n",
    "        file_ext = os.path.splitext(file_path)[1].lower()\n",
    "        video_formats = ['.mp4', '.avi', '.mov', '.mkv', '.webm', '.flv']\n",
    "        audio_formats = ['.mp3', '.wav', '.m4a', '.flac', '.aac', '.ogg']\n",
    "        \n",
    "        audio_path = file_path\n",
    "        audio_duration = 0\n",
    "        \n",
    "        # If it's a video file, extract audio first\n",
    "        if file_ext in video_formats:\n",
    "            print(f\"üìπ Extracting audio from video: {os.path.basename(file_path)}\")\n",
    "            audio_path = file_path.rsplit('.', 1)[0] + '_temp_audio.wav'\n",
    "            success, duration = extract_audio_from_video(file_path, audio_path)\n",
    "            if not success:\n",
    "                return None, None, 0, 0, None, 0\n",
    "            audio_duration = duration\n",
    "        elif file_ext in audio_formats:\n",
    "            print(f\"üéµ Processing audio file: {os.path.basename(file_path)}\")\n",
    "            # Get audio duration using whisper\n",
    "            try:\n",
    "                audio = whisper.load_audio(file_path)\n",
    "                audio_duration = len(audio) / whisper.audio.SAMPLE_RATE\n",
    "            except:\n",
    "                audio_duration = 0\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è  Unsupported file format: {file_ext}\")\n",
    "            return None, None, 0, 0, None, 0\n",
    "        \n",
    "        # Transcribe using Whisper with word-level timestamps\n",
    "        print(f\"üîÑ Transcribing: {os.path.basename(file_path)}\")\n",
    "        result = model.transcribe(audio_path, word_timestamps=True)\n",
    "        \n",
    "        # Initialize speaker diarization variables\n",
    "        transcript_with_speakers = None\n",
    "        speaker_count = 0\n",
    "        \n",
    "        # Perform speaker diarization if available\n",
    "        if DIARIZATION_AVAILABLE and diarization_pipeline is not None:\n",
    "            try:\n",
    "                print(f\"üë• Identifying speakers...\")\n",
    "                diarization = diarization_pipeline(audio_path)\n",
    "                \n",
    "                # Create speaker segments\n",
    "                speaker_segments = []\n",
    "                current_segments = {}\n",
    "                \n",
    "                # Process word-level timestamps with speaker information\n",
    "                for segment in result[\"segments\"]:\n",
    "                    for word_info in segment.get(\"words\", []):\n",
    "                        word_start = word_info[\"start\"]\n",
    "                        word_end = word_info[\"end\"]\n",
    "                        word_text = word_info[\"word\"]\n",
    "                        \n",
    "                        # Find speaker for this time segment\n",
    "                        speaker_label = \"Unknown\"\n",
    "                        for turn, _, speaker in diarization.itertracks(yield_label=True):\n",
    "                            if word_start >= turn.start and word_end <= turn.end:\n",
    "                                speaker_label = f\"Speaker_{speaker}\"\n",
    "                                break\n",
    "                        \n",
    "                        # Group consecutive words by speaker\n",
    "                        if speaker_label not in current_segments:\n",
    "                            current_segments[speaker_label] = {\n",
    "                                \"speaker\": speaker_label,\n",
    "                                \"start_time\": word_start,\n",
    "                                \"end_time\": word_end,\n",
    "                                \"text\": word_text\n",
    "                            }\n",
    "                        else:\n",
    "                            # Extend current segment\n",
    "                            current_segments[speaker_label][\"end_time\"] = word_end\n",
    "                            current_segments[speaker_label][\"text\"] += word_text\n",
    "                \n",
    "                # Convert to final format\n",
    "                for speaker_info in current_segments.values():\n",
    "                    speaker_segments.append({\n",
    "                        \"speaker\": speaker_info[\"speaker\"],\n",
    "                        \"start_time\": round(speaker_info[\"start_time\"], 2),\n",
    "                        \"end_time\": round(speaker_info[\"end_time\"], 2),\n",
    "                        \"duration\": round(speaker_info[\"end_time\"] - speaker_info[\"start_time\"], 2),\n",
    "                        \"text\": speaker_info[\"text\"].strip()\n",
    "                    })\n",
    "                \n",
    "                # Sort by start time\n",
    "                speaker_segments.sort(key=lambda x: x[\"start_time\"])\n",
    "                \n",
    "                # Create JSON structure\n",
    "                transcript_with_speakers = {\n",
    "                    \"file_info\": {\n",
    "                        \"filename\": os.path.basename(file_path),\n",
    "                        \"duration\": round(audio_duration, 2),\n",
    "                        \"language\": result.get('language', 'unknown')\n",
    "                    },\n",
    "                    \"speakers\": speaker_segments,\n",
    "                    \"full_transcript\": result['text'].strip()\n",
    "                }\n",
    "                \n",
    "                speaker_count = len(set(seg[\"speaker\"] for seg in speaker_segments))\n",
    "                print(f\"   üéØ Identified {speaker_count} speakers\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"   ‚ö†Ô∏è  Speaker diarization failed: {e}\")\n",
    "                print(f\"   Continuing with basic transcription...\")\n",
    "        else:\n",
    "            # Create simple JSON structure with time-based segments (fallback)\n",
    "            if result.get(\"segments\"):\n",
    "                segments = []\n",
    "                for i, segment in enumerate(result[\"segments\"]):\n",
    "                    segments.append({\n",
    "                        \"speaker\": f\"Speaker_{i % 2}\",  # Simple alternating speakers as demo\n",
    "                        \"start_time\": round(segment[\"start\"], 2),\n",
    "                        \"end_time\": round(segment[\"end\"], 2), \n",
    "                        \"duration\": round(segment[\"end\"] - segment[\"start\"], 2),\n",
    "                        \"text\": segment[\"text\"].strip()\n",
    "                    })\n",
    "                \n",
    "                transcript_with_speakers = {\n",
    "                    \"file_info\": {\n",
    "                        \"filename\": os.path.basename(file_path),\n",
    "                        \"duration\": round(audio_duration, 2),\n",
    "                        \"language\": result.get('language', 'unknown')\n",
    "                    },\n",
    "                    \"speakers\": segments,\n",
    "                    \"full_transcript\": result['text'].strip()\n",
    "                }\n",
    "                \n",
    "                speaker_count = 2  # Demo: assume 2 speakers\n",
    "                print(f\"   üìù Created demo speaker segments (alternating speakers)\")\n",
    "        \n",
    "        # Clean up temporary audio file if created\n",
    "        if audio_path != file_path and os.path.exists(audio_path):\n",
    "            os.remove(audio_path)\n",
    "        \n",
    "        processing_time = time.time() - start_time\n",
    "        \n",
    "        print(f\"‚úÖ Completed: {os.path.basename(file_path)} ({processing_time:.2f}s)\")\n",
    "        print(f\"   Language: {result.get('language', 'unknown')}\")\n",
    "        print(f\"   Duration: {audio_duration:.1f}s\")\n",
    "        if speaker_count > 0:\n",
    "            print(f\"   Speakers: {speaker_count}\")\n",
    "        \n",
    "        return (\n",
    "            result['text'].strip(),\n",
    "            result.get('language', 'unknown'),\n",
    "            processing_time,\n",
    "            audio_duration,\n",
    "            transcript_with_speakers,\n",
    "            speaker_count\n",
    "        )\n",
    "        \n",
    "    except Exception as e:\n",
    "        processing_time = time.time() - start_time\n",
    "        print(f\"‚ùå Error transcribing {file_path}: {e}\")\n",
    "        return None, None, processing_time, 0, None, 0\n",
    "\n",
    "print(\"‚úÖ Helper functions defined!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce110000-1111-2222-3333-ffffff000018",
   "metadata": {
    "name": "cell19"
   },
   "source": [
    "## 5. Process Media Files\n",
    "\n",
    "Find all media files and process them for transcription.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce110000-1111-2222-3333-ffffff000019",
   "metadata": {
    "language": "python",
    "name": "process_media_files"
   },
   "outputs": [],
   "source": [
    "# Find all media files\n",
    "audio_formats = ['*.mp3', '*.wav', '*.m4a', '*.flac', '*.aac', '*.ogg']\n",
    "video_formats = ['*.mp4', '*.avi', '*.mov', '*.mkv', '*.webm', '*.flv']\n",
    "\n",
    "# Video processing available via FFmpeg CLI\n",
    "supported_formats = audio_formats + video_formats\n",
    "print(\"üìÅ Scanning for audio and video files...\")\n",
    "\n",
    "media_files = []\n",
    "for pattern in supported_formats:\n",
    "    media_files.extend(glob.glob(f'media_files/{pattern}'))\n",
    "    # Also check for uppercase extensions\n",
    "    media_files.extend(glob.glob(f'media_files/{pattern.upper()}'))\n",
    "\n",
    "print(f\"üìÅ Found {len(media_files)} media files to process:\")\n",
    "for i, file in enumerate(media_files[:10], 1):  # Show first 10\n",
    "    file_ext = os.path.splitext(file)[1].lower()\n",
    "    file_type = \"üéµ\" if file_ext in ['.mp3', '.wav', '.m4a', '.flac', '.aac', '.ogg'] else \"üìπ\"\n",
    "    print(f\"   {i}. {file_type} {os.path.basename(file)}\")\n",
    "if len(media_files) > 10:\n",
    "    print(f\"   ... and {len(media_files) - 10} more files\")\n",
    "\n",
    "if len(media_files) == 0:\n",
    "    print(\"‚ö†Ô∏è  No media files found! Please upload files to the AUDIO_VIDEO_STAGE first.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce110000-1111-2222-3333-ffffff000020",
   "metadata": {
    "name": "cell21"
   },
   "source": [
    "## 6. Batch Transcription Processing\n",
    "\n",
    "Process all media files and collect results. This may take some time depending on the number and size of files.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce110000-1111-2222-3333-ffffff000021",
   "metadata": {
    "language": "python",
    "name": "batch_transcription_processing"
   },
   "outputs": [],
   "source": [
    "# Process all media files\n",
    "all_transcriptions = []\n",
    "total_files = len(media_files)\n",
    "failed_files = []\n",
    "\n",
    "if total_files > 0:\n",
    "    print(f\"üöÄ Starting transcription of {total_files} files...\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for i, file_path in enumerate(media_files, 1):\n",
    "        print(f\"\\n[{i}/{total_files}] Processing: {os.path.basename(file_path)}\")\n",
    "        \n",
    "        # Get file info\n",
    "        file_name = os.path.basename(file_path)\n",
    "        file_type = os.path.splitext(file_name)[1][1:].upper()  # Remove dot and uppercase\n",
    "        file_size = get_file_size(file_path)\n",
    "        \n",
    "        # Transcribe the file with speaker diarization\n",
    "        transcript, language, processing_time, audio_duration, transcript_with_speakers, speaker_count = transcribe_media_file(file_path)\n",
    "        \n",
    "        if transcript is not None:\n",
    "            # Store successful transcription\n",
    "            record = {\n",
    "                'FILE_PATH': file_path if INCLUDE_FILE_PATH else '',\n",
    "                'FILE_NAME': file_name,\n",
    "                'FILE_TYPE': file_type,\n",
    "                'DETECTED_LANGUAGE': language,\n",
    "                'TRANSCRIPT': transcript,\n",
    "                'TRANSCRIPT_WITH_SPEAKERS': transcript_with_speakers,\n",
    "                'PROCESSING_TIME_SECONDS': processing_time,\n",
    "                'FILE_SIZE_BYTES': file_size,\n",
    "                'AUDIO_DURATION_SECONDS': audio_duration,\n",
    "                'SPEAKER_COUNT': speaker_count,\n",
    "                'TRANSCRIPTION_TIMESTAMP': datetime.now()\n",
    "            }\n",
    "            all_transcriptions.append(record)\n",
    "        else:\n",
    "            failed_files.append(file_name)\n",
    "        \n",
    "        # Progress update (using configured interval)\n",
    "        if i % PROGRESS_UPDATE_INTERVAL == 0 or i == total_files:\n",
    "            success_count = len(all_transcriptions)\n",
    "            print(f\"\\nüìä Progress: {i}/{total_files} processed, {success_count} successful\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(f\"‚úÖ Transcription complete!\")\n",
    "    print(f\"   Successfully processed: {len(all_transcriptions)} files\")\n",
    "    print(f\"   Failed: {len(failed_files)} files\")\n",
    "    \n",
    "    if failed_files:\n",
    "        print(f\"   Failed files: {', '.join(failed_files)}\")\n",
    "        \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No files to process.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce110000-1111-2222-3333-ffffff000022",
   "metadata": {
    "name": "cell23"
   },
   "source": [
    "## 7. Preview Results\n",
    "\n",
    "Let's take a look at some of the transcription results before storing them in Snowflake.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce110000-1111-2222-3333-ffffff000023",
   "metadata": {
    "language": "python",
    "name": "preview_results"
   },
   "outputs": [],
   "source": [
    "if all_transcriptions:\n",
    "    print(\"üìã Sample transcription results:\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    for i, result in enumerate(all_transcriptions[:3], 1):  # Show first 3 results\n",
    "        print(f\"\\nüéµ File {i}: {result['FILE_NAME']}\")\n",
    "        print(f\"   Type: {result['FILE_TYPE']} | Language: {result['DETECTED_LANGUAGE']}\")\n",
    "        print(f\"   Duration: {result['AUDIO_DURATION_SECONDS']:.1f}s | Processing: {result['PROCESSING_TIME_SECONDS']:.1f}s\")\n",
    "        print(f\"   Size: {result['FILE_SIZE_BYTES']:,} bytes\")\n",
    "        \n",
    "        # Show speaker information if available\n",
    "        if result['SPEAKER_COUNT'] > 0:\n",
    "            print(f\"   üë• Speakers identified: {result['SPEAKER_COUNT']}\")\n",
    "            \n",
    "            # Show first few speaker segments\n",
    "            if result['TRANSCRIPT_WITH_SPEAKERS']:\n",
    "                speakers_data = result['TRANSCRIPT_WITH_SPEAKERS']\n",
    "                print(f\"   üìù Speaker segments preview:\")\n",
    "                for j, segment in enumerate(speakers_data.get('speakers', [])[:3]):\n",
    "                    print(f\"      {segment['speaker']} ({segment['start_time']:.1f}s-{segment['end_time']:.1f}s): {segment['text'][:100]}...\")\n",
    "                if len(speakers_data.get('speakers', [])) > 3:\n",
    "                    print(f\"      ... and {len(speakers_data.get('speakers', [])) - 3} more segments\")\n",
    "        else:\n",
    "            print(f\"   üìù Transcript preview: {result['TRANSCRIPT'][:200]}...\")\n",
    "        \n",
    "        print(\"-\" * 80)\n",
    "    \n",
    "    if len(all_transcriptions) > 3:\n",
    "        print(f\"\\n... and {len(all_transcriptions) - 3} more transcriptions\")\n",
    "        \n",
    "    # Summary statistics\n",
    "    total_duration = sum(r['AUDIO_DURATION_SECONDS'] for r in all_transcriptions)\n",
    "    total_processing = sum(r['PROCESSING_TIME_SECONDS'] for r in all_transcriptions)\n",
    "    avg_speed_ratio = total_processing / total_duration if total_duration > 0 else 0\n",
    "    \n",
    "    languages = {}\n",
    "    file_types = {}\n",
    "    for r in all_transcriptions:\n",
    "        languages[r['DETECTED_LANGUAGE']] = languages.get(r['DETECTED_LANGUAGE'], 0) + 1\n",
    "        file_types[r['FILE_TYPE']] = file_types.get(r['FILE_TYPE'], 0) + 1\n",
    "    \n",
    "    print(f\"\\nüìä Summary Statistics:\")\n",
    "    print(f\"   Total audio duration: {total_duration/60:.1f} minutes\")\n",
    "    print(f\"   Total processing time: {total_processing/60:.1f} minutes\")\n",
    "    print(f\"   Average speed ratio: {avg_speed_ratio:.2f}x (lower is faster)\")\n",
    "    print(f\"   Languages detected: {', '.join(languages.keys())}\")\n",
    "    print(f\"   File types: {', '.join(file_types.keys())}\")\n",
    "else:\n",
    "    print(\"No transcription results to display.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce110000-1111-2222-3333-ffffff000024",
   "metadata": {
    "name": "cell25"
   },
   "source": [
    "## 8. Store Results in Snowflake\n",
    "\n",
    "Now we'll store all transcription results in the Snowflake table for analysis and search.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce110000-1111-2222-3333-ffffff000025",
   "metadata": {
    "language": "python",
    "name": "store_results"
   },
   "outputs": [],
   "source": [
    "if all_transcriptions:\n",
    "    print(\"üíæ Storing transcription results in Snowflake...\")\n",
    "    \n",
    "    # Convert to Pandas DataFrame and fix timestamp formatting\n",
    "    df_results = pd.DataFrame(all_transcriptions)\n",
    "    \n",
    "    # Ensure timestamp is properly formatted as string for Snowflake\n",
    "    df_results['TRANSCRIPTION_TIMESTAMP'] = df_results['TRANSCRIPTION_TIMESTAMP'].dt.strftime('%Y-%m-%d %H:%M:%S.%f')\n",
    "    \n",
    "    # Convert speaker data to JSON strings for storage\n",
    "    df_results['TRANSCRIPT_WITH_SPEAKERS'] = df_results['TRANSCRIPT_WITH_SPEAKERS'].apply(\n",
    "        lambda x: json.dumps(x) if x is not None else None\n",
    "    )\n",
    "    \n",
    "    # Convert to Snowpark DataFrame with proper schema\n",
    "    schema = StructType([\n",
    "        StructField(\"FILE_PATH\", StringType()),\n",
    "        StructField(\"FILE_NAME\", StringType()),\n",
    "        StructField(\"FILE_TYPE\", StringType()),\n",
    "        StructField(\"DETECTED_LANGUAGE\", StringType()),\n",
    "        StructField(\"TRANSCRIPT\", StringType()),\n",
    "        StructField(\"TRANSCRIPT_WITH_SPEAKERS\", StringType()),  # JSON as string\n",
    "        StructField(\"PROCESSING_TIME_SECONDS\", FloatType()),\n",
    "        StructField(\"FILE_SIZE_BYTES\", IntegerType()),\n",
    "        StructField(\"AUDIO_DURATION_SECONDS\", FloatType()),\n",
    "        StructField(\"SPEAKER_COUNT\", IntegerType()),\n",
    "        StructField(\"TRANSCRIPTION_TIMESTAMP\", StringType())  # Use StringType for timestamp\n",
    "    ])\n",
    "    \n",
    "    snowpark_df = session.create_dataframe(df_results, schema=schema)\n",
    "    \n",
    "    # Write to Snowflake table using SQL INSERT with proper timestamp conversion\n",
    "    try:\n",
    "        # Create a temporary view from the DataFrame\n",
    "        snowpark_df.create_or_replace_temp_view(\"temp_transcription_data\")\n",
    "        \n",
    "        # Insert data with proper timestamp and JSON conversion\n",
    "        insert_sql = \"\"\"\n",
    "        INSERT INTO TRANSCRIPTION_RESULTS (\n",
    "            FILE_PATH, FILE_NAME, FILE_TYPE, DETECTED_LANGUAGE, TRANSCRIPT, TRANSCRIPT_WITH_SPEAKERS,\n",
    "            PROCESSING_TIME_SECONDS, FILE_SIZE_BYTES, AUDIO_DURATION_SECONDS, SPEAKER_COUNT, TRANSCRIPTION_TIMESTAMP\n",
    "        )\n",
    "        SELECT \n",
    "            FILE_PATH, FILE_NAME, FILE_TYPE, DETECTED_LANGUAGE, TRANSCRIPT,\n",
    "            CASE \n",
    "                WHEN TRANSCRIPT_WITH_SPEAKERS IS NOT NULL \n",
    "                THEN PARSE_JSON(TRANSCRIPT_WITH_SPEAKERS)\n",
    "                ELSE NULL \n",
    "            END as TRANSCRIPT_WITH_SPEAKERS,\n",
    "            PROCESSING_TIME_SECONDS, FILE_SIZE_BYTES, AUDIO_DURATION_SECONDS, SPEAKER_COUNT,\n",
    "            TO_TIMESTAMP_NTZ(TRANSCRIPTION_TIMESTAMP, 'YYYY-MM-DD HH24:MI:SS.FF6')\n",
    "        FROM temp_transcription_data\n",
    "        \"\"\"\n",
    "        \n",
    "        result = session.sql(insert_sql)\n",
    "        result.collect()  # Execute the query\n",
    "        \n",
    "        print(f\"‚úÖ Successfully stored {len(all_transcriptions)} transcription records!\")\n",
    "        \n",
    "        # Verify the data was written\n",
    "        count_query = session.sql(\"SELECT COUNT(*) as TOTAL FROM TRANSCRIPTION_RESULTS\")\n",
    "        total_records = count_query.collect()[0]['TOTAL']\n",
    "        print(f\"   Total records in TRANSCRIPTION_RESULTS table: {total_records}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error storing results: {e}\")\n",
    "        print(\"   Trying alternative method...\")\n",
    "        \n",
    "        # Alternative method: Insert records one by one\n",
    "        try:\n",
    "            for record in all_transcriptions:\n",
    "                # Handle JSON speaker data\n",
    "                speaker_json = \"NULL\"\n",
    "                if record['TRANSCRIPT_WITH_SPEAKERS'] is not None:\n",
    "                    # Escape the JSON for SQL\n",
    "                    json_str = json.dumps(record['TRANSCRIPT_WITH_SPEAKERS']).replace(\"'\", \"''\")\n",
    "                    speaker_json = f\"PARSE_JSON('{json_str}')\"\n",
    "                \n",
    "                insert_sql = f\"\"\"\n",
    "                INSERT INTO TRANSCRIPTION_RESULTS VALUES (\n",
    "                    '{record['FILE_PATH']}',\n",
    "                    '{record['FILE_NAME']}',\n",
    "                    '{record['FILE_TYPE']}',\n",
    "                    '{record['DETECTED_LANGUAGE']}',\n",
    "                    $${record['TRANSCRIPT']}$$,\n",
    "                    {speaker_json},\n",
    "                    {record['PROCESSING_TIME_SECONDS']},\n",
    "                    {record['FILE_SIZE_BYTES']},\n",
    "                    {record['AUDIO_DURATION_SECONDS']},\n",
    "                    {record['SPEAKER_COUNT']},\n",
    "                    CURRENT_TIMESTAMP()\n",
    "                )\n",
    "                \"\"\"\n",
    "                session.sql(insert_sql).collect()\n",
    "            \n",
    "            print(f\"‚úÖ Successfully stored {len(all_transcriptions)} records using alternative method!\")\n",
    "            \n",
    "        except Exception as e2:\n",
    "            print(f\"‚ùå Alternative method also failed: {e2}\")\n",
    "        \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No transcription results to store.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce110000-1111-2222-3333-ffffff000026",
   "metadata": {
    "name": "cell27"
   },
   "source": [
    "## 9. Query and Analyze Results\n",
    "\n",
    "Let's run some sample queries to explore our transcription data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce110000-1111-2222-3333-ffffff000027",
   "metadata": {
    "language": "python",
    "name": "query_and_analyze_results"
   },
   "outputs": [],
   "source": [
    "# Query recent transcriptions\n",
    "print(\"üîç Recent transcriptions:\")\n",
    "recent_query = \"\"\"\n",
    "SELECT \n",
    "    FILE_NAME,\n",
    "    FILE_TYPE,\n",
    "    DETECTED_LANGUAGE,\n",
    "    ROUND(AUDIO_DURATION_SECONDS, 1) as DURATION_SEC,\n",
    "    ROUND(PROCESSING_TIME_SECONDS, 1) as PROCESSING_SEC,\n",
    "    TRANSCRIPTION_TIMESTAMP\n",
    "FROM TRANSCRIPTION_RESULTS \n",
    "ORDER BY TRANSCRIPTION_TIMESTAMP DESC \n",
    "LIMIT 5\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    recent_df = session.sql(recent_query).to_pandas()\n",
    "    if not recent_df.empty:\n",
    "        print(recent_df.to_string(index=False))\n",
    "    else:\n",
    "        print(\"No results found.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error querying data: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce110000-1111-2222-3333-ffffff000028",
   "metadata": {
    "language": "python",
    "name": "summary_statistics"
   },
   "outputs": [],
   "source": [
    "# Query summary statistics using the view\n",
    "print(\"üìä Summary statistics from TRANSCRIPTION_SUMMARY view:\")\n",
    "summary_query = \"SELECT * FROM TRANSCRIPTION_SUMMARY ORDER BY FILE_COUNT DESC\"\n",
    "\n",
    "try:\n",
    "    summary_df = session.sql(summary_query).to_pandas()\n",
    "    if not summary_df.empty:\n",
    "        print(summary_df.to_string(index=False))\n",
    "    else:\n",
    "        print(\"No summary data available.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error querying summary: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce110000-1111-2222-3333-ffffff000029",
   "metadata": {
    "language": "python",
    "name": "search_transcriptions"
   },
   "outputs": [],
   "source": [
    "# Search transcriptions for specific content (example)\n",
    "search_term = \"meeting\"  # Change this to search for different terms\n",
    "print(f\"üîç Searching for '{search_term}' in transcriptions:\")\n",
    "\n",
    "search_query = f\"\"\"\n",
    "SELECT \n",
    "    FILE_NAME,\n",
    "    DETECTED_LANGUAGE,\n",
    "    SPEAKER_COUNT,\n",
    "    SUBSTR(TRANSCRIPT, 1, 100) as TRANSCRIPT_PREVIEW\n",
    "FROM TRANSCRIPTION_RESULTS \n",
    "WHERE TRANSCRIPT ILIKE '%{search_term}%'\n",
    "LIMIT 3\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    search_df = session.sql(search_query).to_pandas()\n",
    "    if not search_df.empty:\n",
    "        print(search_df.to_string(index=False))\n",
    "    else:\n",
    "        print(f\"No transcriptions found containing '{search_term}'\")\n",
    "except Exception as e:\n",
    "    print(f\"Error searching transcriptions: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "# Example: Query speaker segments from JSON data\n",
    "print(\"üë• Example: Querying speaker segments from JSON data:\")\n",
    "\n",
    "speaker_query = \"\"\"\n",
    "SELECT \n",
    "    FILE_NAME,\n",
    "    SPEAKER_COUNT,\n",
    "    TRANSCRIPT_WITH_SPEAKERS:file_info:language::STRING as LANGUAGE,\n",
    "    TRANSCRIPT_WITH_SPEAKERS:speakers[0]:speaker::STRING as FIRST_SPEAKER,\n",
    "    TRANSCRIPT_WITH_SPEAKERS:speakers[0]:text::STRING as FIRST_SPEAKER_TEXT\n",
    "FROM TRANSCRIPTION_RESULTS \n",
    "WHERE TRANSCRIPT_WITH_SPEAKERS IS NOT NULL\n",
    "LIMIT 2\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    speaker_df = session.sql(speaker_query).to_pandas()\n",
    "    if not speaker_df.empty:\n",
    "        print(speaker_df.to_string(index=False))\n",
    "    else:\n",
    "        print(\"No files with speaker data found\")\n",
    "except Exception as e:\n",
    "    print(f\"Error querying speaker data: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce110000-1111-2222-3333-ffffff000030",
   "metadata": {
    "name": "cell31"
   },
   "source": [
    "## 10. Cleanup and Next Steps\n",
    "\n",
    "Clean up temporary files and provide information about next steps.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce110000-1111-2222-3333-ffffff000031",
   "metadata": {
    "language": "python",
    "name": "cleanup"
   },
   "outputs": [],
   "source": [
    "# Cleanup temporary files\n",
    "import shutil\n",
    "\n",
    "try:\n",
    "    if os.path.exists('media_files'):\n",
    "        shutil.rmtree('media_files')\n",
    "        print(\"üßπ Cleaned up temporary media files\")\n",
    "except Exception as e:\n",
    "    print(f\"Note: Could not clean up temporary files: {e}\")\n",
    "\n",
    "print(\"\\nüéâ Transcription process complete!\")\n",
    "print(\"\\nüìã What you can do next:\")\n",
    "print(\"   1. Use the Streamlit dashboard to explore and search your transcriptions\")\n",
    "print(\"   2. Run SQL queries against the TRANSCRIPTION_RESULTS table\")\n",
    "print(\"   3. Use the TRANSCRIPTION_SUMMARY view for analytics\")\n",
    "print(\"   4. Set up automated processing with Snowflake Tasks (see README)\")\n",
    "print(\"\\nüí° Tips:\")\n",
    "print(\"   ‚Ä¢ Upload more files to AUDIO_VIDEO_STAGE and re-run this notebook\")\n",
    "print(\"   ‚Ä¢ Experiment with different Whisper models (tiny, base, small, medium, large)\")\n",
    "print(\"   ‚Ä¢ Use the search functionality to find specific content in transcriptions\")\n",
    "print(\"\\nüìä Dashboard: Run 'streamlit run streamlit/transcription_dashboard.py'\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "lastEditStatus": {
   "authorEmail": "bo.landsman@snowflake.com",
   "authorId": "1929370550625",
   "authorName": "BOLANDSMAN",
   "lastEditTime": 1754069266620,
   "notebookId": "qwkq4zj3p5e3cthmtjaw",
   "sessionId": "a39b73c0-3f31-4414-ab57-f22160584287"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
